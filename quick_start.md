# Quick Start Guide

Get your food ordering chatbot running in 10 minutes!

## Prerequisites

- Ubuntu/Linux machine (or WSL on Windows)
- Docker and Docker Compose installed
- 8GB RAM minimum (16GB recommended for Ollama)
- Internet connection

## Installation Steps

### 1. Clone and Setup

```bash
# Create project directory
mkdir food-ordering-bot
cd food-ordering-bot

# Create necessary files (menu.json, docker-compose.yml, etc)
# Copy all artifact files from the task submission
```

### 2. Start Services

```bash
# Make setup script executable
chmod +x setup.sh

# Run automated setup
./setup.sh
```

**What the script does:**
- Starts n8n and Ollama containers
- Pulls llama3.1:8b model (~4.7GB)
- Configures ngrok tunnel
- Sets Telegram webhook automatically

**Expected output:**
```
=== Food Ordering Chatbot Setup ===
Step 1: Starting Docker containers...
Step 2: Waiting for services to start...
Step 3: Pulling Ollama model...
Step 4: Setting up ngrok...
Step 5: Setting Telegram webhook...

=== Setup Complete! ===
Your ngrok URL: https://abc123.ngrok.io
Webhook URL: https://abc123.ngrok.io/webhook/telegram
n8n Interface: http://localhost:5678
```

### 3. Import Workflow

1. Open http://localhost:5678 in your browser
2. Click "Import from File"
3. Select `n8n-workflow-simplified.json`
4. The workflow will be imported with all nodes

### 4. Update Webhook URL (Important!)

In the imported workflow:
1. Find the "Telegram Webhook" node
2. Note the path: `/webhook/telegram`
3. Your full webhook URL is: `YOUR_NGROK_URL/webhook/telegram`

### 5. Activate Workflow

1. Click the toggle switch at the top right
2. Workflow should show "Active" status
3. Green checkmark means it's working

### 6. Test the Bot

1. Open Telegram
2. Search for your bot (use the bot username)
3. Send `/start`
4. You should see the welcome message with buttons!

## Quick Test Commands

```bash
# Test 1: Check webhook status
curl https://api.telegram.org/bot8246352423:AAGe6R3M5VLUXzlWMmFdccQMi2QuNebdZN4/getWebhookInfo

# Expected: "ok": true, "pending_update_count": 0

# Test 2: Check Ollama
curl http://localhost:11434/v1/models

# Expected: List containing llama3.1:8b

# Test 3: Check ngrok
curl http://localhost:4040/api/tunnels

# Expected: JSON with your public URL

# Test 4: Check n8n
curl http://localhost:5678/healthz

# Expected: {"status": "ok"}
```

## Common Issues & Fixes

### Issue 1: Bot Not Responding

**Symptom:** Send `/start`, no response

**Fix:**
```bash
# Check webhook
curl https://api.telegram.org/bot8246352423:AAGe6R3M5VLUXzlWMmFdccQMi2QuNebdZN4/getWebhookInfo

# If pending_update_count > 0, reset webhook:
curl -X POST https://api.telegram.org/bot8246352423:AAGe6R3M5VLUXzlWMmFdccQMi2QuNebdZN4/deleteWebhook
./setup.sh  # Run setup again
```

### Issue 2: ngrok Expired

**Symptom:** Webhook fails after 2 hours

**Fix:**
```bash
# Restart ngrok
pkill ngrok
ngrok http 5678 &

# Get new URL
NEW_URL=$(curl -s http://localhost:4040/api/tunnels | jq -r '.tunnels[0].public_url')

# Update webhook
curl -X POST https://api.telegram.org/bot8246352423:AAGe6R3M5VLUXzlWMmFdccQMi2QuNebdZN4/setWebhook \
  -d "url=$NEW_URL/webhook/telegram"
```

### Issue 3: AI Not Working

**Symptom:** Structured codes work, but free text fails

**Fix:**
```bash
# Check Ollama
docker logs ollama

# Restart Ollama
docker restart ollama

# Pull model again if needed
docker exec ollama ollama pull llama3.1:8b

# Test directly
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"llama3.1:8b","messages":[{"role":"user","content":"test"}]}'
```

### Issue 4: n8n Workflow Errors

**Symptom:** Workflow shows errors in execution

**Fix:**
```bash
# Check n8n logs
docker logs n8n -f

# Restart n8n
docker restart n8n

# Re-import workflow
# Open http://localhost:5678
# Delete old workflow
# Import fresh copy
```

## Usage Examples

### Example 1: Structured Ordering
```
You: BG1 x2, SD1, DR1
Bot: ‚úÖ ÿ™ŸÖ ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑÿπŸÜÿßÿµÿ± ŸÑŸÑÿ≥ŸÑÿ©:
     ‚Ä¢ Classic Burger x2 - 170 EGP
     ‚Ä¢ French Fries x1 - 30 EGP
     ‚Ä¢ Coca Cola x1 - 15 EGP
     üí∞ ÿßŸÑÿ•ÿ¨ŸÖÿßŸÑŸä: 215 EGP
```

### Example 2: Natural Language
```
You: I want two chicken burgers and fries
Bot: ü§ñ ÿ¨ÿßÿ±Ÿä ŸÖÿπÿßŸÑÿ¨ÿ© ÿ∑ŸÑÿ®ŸÉ...
     ‚úÖ ÿ™ŸÖ ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑÿπŸÜÿßÿµÿ± ŸÑŸÑÿ≥ŸÑÿ©:
     ‚Ä¢ Chicken Burger x2 - 150 EGP
     ‚Ä¢ French Fries x1 - 30 EGP
     üí∞ ÿßŸÑÿ•ÿ¨ŸÖÿßŸÑŸä: 180 EGP
```

### Example 3: Arabic
```
You: ÿπÿßŸäÿ≤ ÿ®ÿ±ÿ¨ÿ± ŸÉŸÑÿßÿ≥ŸäŸÉ ŸàŸÉŸàŸÑÿß
Bot: ‚úÖ ÿ™ŸÖ ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑÿπŸÜÿßÿµÿ± ŸÑŸÑÿ≥ŸÑÿ©:
     ‚Ä¢ Classic Burger x1 - 85 EGP
     ‚Ä¢ Coca Cola x1 - 15 EGP
```

## Next Steps

### Add More Features

1. **Menu Management**: Create admin interface to update menu
2. **Payment Integration**: Add Fawry/Paymob
3. **Order Tracking**: Real-time status updates
4. **Analytics**: Track popular items

### Production Deployment

1. **Get a Domain**: Replace ngrok with real domain
2. **SSL Certificate**: Use Let's Encrypt
3. **Cloud Hosting**: Deploy to AWS/GCP/Azure
4. **Monitoring**: Set up Sentry/Datadog
5. **Backups**: Automated Data Store backups

### Customize

1. **Change Menu**: Edit `menu.json`
2. **Update Prices**: Modify prices in menu
3. **Add Languages**: Extend translation support
4. **Custom Branding**: Update bot messages and emojis

## Monitoring

### Check Service Status
```bash
# All services
docker-compose ps

# n8n logs
docker logs n8n -f

# Ollama logs
docker logs ollama -f

# ngrok status
curl http://localhost:4040/api/tunnels
```

### Performance Monitoring
```bash
# Docker stats
docker stats

# Disk usage
df -h

# Memory usage
free -h
```

## Backup & Restore

### Backup
```bash
# Backup n8n data
docker cp n8n:/home/node/.n8n ./n8n-backup

# Backup Data Store
# Data Store is inside n8n volume
docker volume inspect n8n_n8n_data
```

### Restore
```bash
# Restore n8n data
docker cp ./n8n-backup n8n:/home/node/.n8n

# Restart n8n
docker restart n8n
```

## Stopping Services

```bash
# Stop all services
docker-compose down

# Stop but keep data
docker-compose stop

# Stop and remove everything
docker-compose down -v
pkill ngrok
```

## Getting Help

### Documentation
- README.md - Full documentation
- NOTES.md - Technical details
- TRANSCRIPT.md - Example conversations
- n8n-workflow-guide.md - Workflow explanation

### Support
- Check logs: `docker-compose logs -f`
- n8n docs: https://docs.n8n.io
- Ollama docs: https://ollama.ai/docs
- Telegram Bot API: https://core.telegram.org/bots/api

### Debugging Tips

1. **Enable verbose logging** in n8n
2. **Check webhook info** regularly
3. **Test AI separately** before workflow
4. **Use n8n's test execution** feature
5. **Check Data Store** contents in n8n UI

## Success Checklist

- [ ] Docker containers running
- [ ] Ollama model downloaded
- [ ] ngrok tunnel active
- [ ] Webhook set successfully
- [ ] n8n workflow imported and active
- [ ] Bot responds to `/start`
- [ ] Structured ordering works (BG1 x2)
- [ ] AI extraction works (free text)
- [ ] Cart operations work
- [ ] Checkout flow completes
- [ ] Order confirmation received

## Performance Expectations

- **Response Time**: 1-2 seconds for structured, 3-8 seconds for AI
- **Concurrent Users**: Supports 50+ simultaneous users
- **Uptime**: 99%+ with proper monitoring
- **Model Inference**: ~500ms per AI request (depends on hardware)

## Resource Usage

- **CPU**: 2-4 cores (peak during AI inference)
- **RAM**: 8GB minimum (16GB recommended)
- **Disk**: 10GB (5GB for Ollama model)
- **Network**: Stable internet for Telegram API

---

üéâ **Congratulations!** Your food ordering bot is now running!

Start chatting with your bot on Telegram and place your first order! üçî
